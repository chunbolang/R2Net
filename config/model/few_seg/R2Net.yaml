

batch_size: 8 # batch size for training (bs8 for 1GPU)
base_lr: 0.005

# epochs: 24
# pretrain: False
fp16: False
# SubEpoch_val: True # val at the half epoch

aux_weight1: 0.5
aux_weight2: 0.5


lr_decay:
  type: 'poly_learning_rate'
  # rate: 0.9  # (1- rate)*lr
  index_split: -1 # index for determining the params group with 10x learning rate
  power: 0.9 # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  warmup: False

freeze_layer: ['layer0', 'layer1', 'layer2', 'layer3', 'layer4', 'base_layer' ]

para_limit:
  name: ['alpha', 'beta', 'pro_global']
  limit: [[0,1], [0,1], [0,1]]
 